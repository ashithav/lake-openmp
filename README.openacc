README.txt
==========
Compile
-------
Change ACC in Makefile to 
ACC=-acc -ta=nvidia
and compile:
#make; make clean

Run the program for Grid=1024x1024, Time=2.0, pebbles = 4, no. of threads=16
#./lake 1024 4 2.0 16

Added the following acc pragma:
#pragma acc kernels loop 
for Loop1 and Loop2

The execution time is: 
./lake 1024 4 2.0 16
Serial:  255.481399 seconds
openACC: 99.566646 seconds

where Serial is the time taken by the initial version of the simulator,
on CPU with one thread. 

openACC hasn't improved the performance much even after running on GPU. 

Setting up the environment variable PGI_ACC_TIME=1, the timing can be 
observed as follows: 

Accelerato Kernel Timing data
/home/temp1007/HW3/Q2/lake-openmp/lake.c
  run_sim_openmp  NVIDIA  devicenum=0
    time(us): 48,641,150
    306: compute region reached 4097 times
        306: data copyin reached 12291 times
             device time(us): total=17,872,999 max=1,546 min=1,452 avg=1,454
        311: kernel launched 4097 times
            grid: [16x256]  block: [64x4]
             device time(us): total=2,266,495 max=662 min=525 avg=553
            elapsed time(us): total=2,301,569 max=672 min=533 avg=561
        334: data copyout reached 4097 times
             device time(us): total=5,230,968 max=1,306 min=1,273 avg=1,276
    334: compute region reached 4097 times
        334: data copyin reached 8194 times
             device time(us): total=11,916,187 max=1,511 min=1,452 avg=1,454
        339: kernel launched 4097 times
            grid: [8x1024]  block: [128]
             device time(us): total=898,194 max=273 min=218 avg=219
            elapsed time(us): total=934,447 max=483 min=225 avg=228
        347: data copyout reached 8194 times
             device time(us): total=10,456,307 max=1,331 min=1,273 avg=1,276

Reduce Data transfers:
----------------------
For every iteration of the while loop, data (uo,uc,pebs) is transferred between 
CPU and GPU as seen from data copyin, data copyout count. These data transfers 
are costly and take time as seen above. 

Since uo, uc is updated after each iteration, the data is copied from
GPU to cpu and then updated back to GPU. This can be avoided. The uo, uc, pebs, 
and un is copied once to the GPU before the first iteration. 

Since the result we need is un[], only un[] is copied back once after the last 
iteration.

To indicate that to the compiler I added this pragma before the while loop
#pragma acc data copyin(uo,uc,pebs), copy(un)

Now the total no. of copyin is reduced to 4 and 
the total no. of copyout is reduced to 1.

With reduced data transfers the execution time is as follows:
./lake 1024 4 2.0 16
Serial:  258.897981 seconds
openACC: 3.739964 seconds

Reducing data transfers improved the performance the most.

Adjust work per thread
----------------------
The default compiler options set the grid dimentions and block dimensions such 
that the total number of threads will be equal to the total number of iterations.
In the above ex: 
number of points = 1024*1024
For loop1, grid: [16x256]  block: [64x4]
For loop2, grid: [8x1024]  block: [128]

The grid, block dimensions can be adjusted so that each thread does more than 
one iteration thus saving time on the amortized cost of setup. But the number of 
the threads must be large enough to fill the GPU to make use it.

GPUs prefer the no. of threads per block to be  a multiple of 32 for better 
performance.

Outer loop
#pragma acc kernels loop gang(16), vector(16)
Inner loop
#pragma acc loop gang(32) vector(16) 

This would setup grid: [16x32]  block: [16x16] = 256*256 threads. For any input
greater than 256, each thread does more than one iteration. 

This optimization did not improve the performance much. 

Cache Data:
----------
Data can be cached in shared memory to reduce general access as follows:

#pragma acc cache(uc[i-1:i+1][j-1:j+1])

This optimization improved the performance by a factor of 2.

Various grid sizes with time = 2.000000 and 4 pebbles 
None- The native acc pragma with no optimization
O1 - Reduced data transfer
O2 - Increased work/thread
O3 - Cached Data
All - All optimizations

Grid      time        time         time       time     
          None         O1         O1+O2       All
---------------------------------------------------
128      0.760248   0.449595    4.483760    1.353219 
256      6.200168   0.538290    4.512954    5.444664 
512     16.069195   1.948370    0.922911    0.976209 
1024   148.099918   3.711453    3.723380    6.265354 

Performance against Simulation time:
-----------------------------------
Grid=1024 and 4 pebbles with varying time
Simulation time     execution time
-----------------------------------
2.0                 4.242537
1.8                 5.724502
1.6                 3.473115  
1.4                 4.765865
1.2                 4.293605
1.0                 3.930076

Performance doesn't vary much with the simulation time.


